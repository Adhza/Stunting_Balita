# -*- coding: utf-8 -*-
"""Stunting Balita With Random Forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YQ4MRd-JAt9FmK2L_EzouQh9uliAK_Vk
"""

import pandas as pd
import numpy as np

from google.colab import files

uploaded = files.upload()

dataraw_file = list(uploaded.keys())[0]

response_data = pd.read_csv('Dataa.csv')
response_data

response_data.dtypes

response_data = response_data.astype({'Label' : 'category'})
response_data = response_data.astype({'Jenis Kelamin Anak' : 'category'})
response_data = response_data.astype({'Pendidikan Terakhir Ayah' : 'category'})
response_data = response_data.astype({'Pendidikan Terakhir Ibu': 'category'})
response_data = response_data.astype({'Berapa kali pemeriksaan kehamilan' : 'category'})
response_data = response_data.astype({'Tempat persalinan' : 'category'})
response_data = response_data.astype({'Anak pernah mengalami ISPA (Infeksi Saluran Pernapasan Akut) seperti batuk. pilek. sinus. dsb?' : 'category'})
response_data = response_data.astype({'Anak pernah mengalami diare?' : 'category'})
response_data = response_data.astype({'Apakah anak batuk dalam 3 bulan terakhir?' : 'category'})
response_data.dtypes

response_data_clear = response_data.drop(["Timestamp", "Email Address"], axis=1)
print(response_data_clear)

#Feature Selection menggunakan chi-square
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

x = response_data_clear.iloc[:,0:22]
y = response_data_clear.iloc[:,-1]

bestfeatures = SelectKBest(score_func=chi2, k=10)
fit= bestfeatures.fit(x,y)

dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(x.columns)

featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']

featureScores

print(featureScores.nlargest(10, 'Score'))

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import Normalizer
from sklearn import preprocessing

x = response_data_clear.iloc[:,0:22]
y = response_data_clear.iloc[:,-1]

model = LogisticRegression(solver='lbfgs')
rfe = RFE(model, 3)
fit = rfe.fit(x, y)

dfscores = pd.DataFrame(fit.ranking_)
dfcolumns = pd.DataFrame(x.columns)

featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']

featureScores
#print(featureScores.nlargest(10, 'Score'))

# feature selection menggunakan extra tress classifier
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt

x = response_data_clear.iloc[:,0:22]
y = response_data_clear.iloc[:,-1]

model = ExtraTreesClassifier()
model.fit(x,y)

print(model.feature_importances_)

feat_importances = pd.Series(model.feature_importances_, index = x.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

# feature selection menggunakan information gain
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import SelectPercentile

x = response_data_clear.iloc[:,0:22]
y = response_data_clear.iloc[:,-1]

MI = mutual_info_classif(x, y)
len(MI)

MI = pd.Series(MI)
MI.index = x.columns
MI.sort_values (ascending = False, inplace = True)
MI.plot.bar(figsize = (10,5))

sel = SelectPercentile(mutual_info_classif, percentile=10)

#split data training dan test
from sklearn.model_selection import train_test_split

x = response_data_clear.drop(["Label"], axis=1)
feature_list = list(x.columns)
x.head()

y = response_data_clear["Label"]
y.head()

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.4)
print(x_train.shape)
print(x_test.shape)

#Scalling Data
from sklearn.preprocessing import StandardScaler

feature_scaler = StandardScaler()
x_train = feature_scaler.fit_transform(x_train)
x_test = feature_scaler.fit_transform(x_test)

#Import random forest classifier
#from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
#Create a Gaussian Classifier
#clf=RandomForestClassifier(max_depth=2, random_state=0)
clf=KNeighborsClassifier(n_neighbors=2)

#Train the model using the training sets y_pred=clf.predict(X_test)
classifier = clf.fit(x_train,y_train)
y_pred = classifier.predict(x_test)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

#Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print(confusion_matrix(y_test, y_pred))
print(y_test.shape)
print(y_pred.shape)

print	('Accuracy	  :',	round(accuracy_score(y_test, y_pred)*100,2),"%")
print ('Precision	  :', round(precision_score(y_test, y_pred, average='weighted', zero_division=0)*100,2),"%")
print ('Recall	    :', round(recall_score(y_test, y_pred, average='weighted', zero_division=0)*100,2),"%")
print	('f1-score	  :',	round(f1_score(y_test,	y_pred, average='weighted', zero_division=0)*100,2), "%")

# hypertuning paramater menggunakan  GridSearch
from sklearn.model_selection import GridSearchCV 
from sklearn.ensemble import RandomForestClassifier

param_grid = {
  'n_estimators': [200, 500, 700], 
  'max_features': ['auto', 'sqrt', 'log2'],
  'max_depth' : [4,5,6,7,8],
  'criterion' :['gini', 'entropy'],
}

rfc=RandomForestClassifier(max_depth=2, random_state=0)
CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv = 5)
CV_rfc.fit(x_train, y_train)
CV_rfc.best_params_

# Grid Search test best parameters
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

CV_rfc.best_params_rfc1=RandomForestClassifier(random_state=0, criterion='gini',	max_depth=4, max_features='auto',	n_estimators=	200)
CV_rfc.best_params_rfc1.fit(x_train, y_train) 
pred=CV_rfc.predict(x_test) 
print (pred)

print (confusion_matrix(y_test, pred))
print ('Accuracy	:'	,	round(accuracy_score(y_test, pred)*100,2),"%")
print ('Precision	:', round(precision_score(y_test, pred, average='weighted', zero_division=0)*100,2),"%")
print ('Recall	:', round(recall_score(y_test, pred, average='weighted', zero_division=0)*100,2), "%")
print	('f1-score	:',	round(f1_score(y_test,	pred, average='weighted', zero_division=0)*100,2), "%")

#split data menggunakan k-fold
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.neighbors import KNeighborsClassifier

kf=KFold(n_splits=10, shuffle = False)
print(kf)  #buat tau Kfold dan parameter defaultnya
i=1        #ini gapenting, cuma buat nandain fold nya.

x = response_data_clear.drop(["Label"], axis=1)
y = response_data_clear["Label"]

feature_scaler = StandardScaler()
x = feature_scaler.fit_transform(x)

def cross_val(estimator):     
  acc = []     
  pcs = []     
  rec = []
  f1s = []
  
  for train_index, test_index in kf.split(x):         
      x_train, x_test = x[train_index], x[test_index]         
      y_train, y_test = y[train_index], y[test_index] 

      model = estimator.fit(x_train, y_train)         
      y_pred = model.predict(x_test)                  
      acc.append(accuracy_score(y_test, y_pred))         
      pcs.append(precision_score(y_test, y_pred, average='macro', zero_division=0))                       
      rec.append(recall_score(y_test, y_pred, average='macro', zero_division=0))
      f1s.append(f1_score(y_test, y_pred, average='macro', zero_division=0))  
      
      print(metrics.classification_report(y_test, y_pred, zero_division=0))
      print(f'confusion matrix:\n {metrics.confusion_matrix(y_test, y_pred)}')        
      print('================================================\n')  

  print(f'average akurasi: {np.mean(acc)}')
  print(f'average precision: {np.mean(pcs)}')
  print(f'average recall: {np.mean(rec)}')
  print(f'average f1-score: {np.mean(f1s)}')  

from sklearn.ensemble import RandomForestClassifier

clf=KNeighborsClassifier(metric='minkowski', n_neighbors=2, weights='uniform')
cross_val(clf)

# hypertuning paramater menggunakan  GridSearch
from sklearn.model_selection import GridSearchCV 
from sklearn.neighbors import KNeighborsClassifier

param_grid = {
  'n_neighbors': [3, 5, 11, 19], 
  'metric': ['minkowski', 'euclidean', 'manhattan'],
  'weights' : ['uniform', 'distance'],
}

knn = KNeighborsClassifier(n_neighbors=2)
CV_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv = 5)
CV_knn.fit(x_train, y_train)
CV_knn.best_params_

# Grid Search test best parameters
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

CV_knn.best_params_knn1= KNeighborsClassifier(metric='minkowski', n_neighbors=5, weights= 'uniform')
CV_knn.best_params_knn1.fit(x_train, y_train) 
pred=CV_knn.predict(x_test) 
print (pred)

print (confusion_matrix(y_test, pred))
print ('Accuracy	:'	,	round(accuracy_score(y_test, pred)*100,2),"%")
print ('Precision	:', round(precision_score(y_test, pred, average='weighted', zero_division=0)*100,2),"%")
print ('Recall	:', round(recall_score(y_test, pred, average='weighted', zero_division=0)*100,2), "%")
print	('f1-score	:',	round(f1_score(y_test,	pred, average='weighted', zero_division=0)*100,2), "%")

# Grid Search test best parameters
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.neighbors import KNeighborsClassifier

CV_knn.best_params_knn1=KNeighborsClassifier(metric='minkowski', n_neighbors=2, weights='uniform')
CV_knn.best_params_knn1.fit(x_train, y_train) 
pred=CV_knn.predict(x_test) 
print (pred)

print (confusion_matrix(y_test, pred))
print ('Accuracy	:'	,	round(accuracy_score(y_test, pred)*100,2),"%")
print ('Precision	:', round(precision_score(y_test, pred, average='weighted')*100,2),"%")
print ('Recall	:', round(recall_score(y_test, pred, average='weighted')*100,2), "%")
print	('f1-score	:',	round(f1_score(y_test,	pred, average='weighted')*100,2), "%")

#print(x_test.shape)
#testing precision
#TP = 47
#FN = 0
#FP = 4
#TN = 0

#precision =TP/(TP+FP)
#precision

#load library
import seaborn as sns
import matplotlib.pyplot as plt

CM = confusion_matrix(y_train, pred)
plt.figure(figsize=(9,5))
axes = sns.heatmap(CM, annot= True)
plt.xlabel('Actual')
plt.ylabel('Predicted Model')
plt.title('Confusion Matrix')

class_Label = ['Data Tidak Stunting','Data Stunting']
class_prediction = ['Data Tidak Stunting','Data Stunting']

tick_marks = np.arange(len(class_Label)) + 0.5

axes.set_xticks(tick_marks + 0.3)
axes.set_xticklabels(class_Label, rotation=0)

axes.set_yticks(tick_marks + 0.3)
axes.set_yticklabels(class_prediction, rotation=0)

plt.show()

#plot tree
# Import tools needed for visualization
from sklearn.tree import export_graphviz
import pydot

# Pull out one tree from the forest
tree = classifier.estimators_[10]

# Import tools needed for visualization
from sklearn.tree import export_graphviz
import pydot

tree = classifier.estimators_[10]

# Export the image to a dot file
export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)

# Use dot file to create a graph
(graph, ) = pydot.graph_from_dot_file('tree.dot')
# Write graph to a png file
graph.write_png('tree.png')